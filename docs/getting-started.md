# Getting Started

This is a guide on how to get started with Cluster API Provider Proxmox (CAPPX). To learn more about cluster API in more
depth, check out the [Cluster API book][cluster-api-book].

- [Getting Started](#getting-started)
    - [Install Requirements](#install-requirements)
        - [Proxmox Requirements](#proxmox-requirements)
            - [Proxmox Credentials](#proxmox-credentials)
            - [Uploading the machine images](#uploading-the-machine-images)
    - [Creating a test management cluster](#creating-a-test-management-cluster)
    - [Configuring and installing Cluster API Provider Proxmox in a management cluster](#configuring-and-installing-cluster-api-provider-proxmox-in-a-management-cluster)
    - [Creating a Proxmox-based workload cluster](#creating-a-proxmox-based-workload-cluster)
    - [Accessing the workload cluster](#accessing-the-workload-cluster)
    - [Custom cluster templates](#custom-cluster-templates)

## Install Requirements

- clusterctl, which can downloaded the latest [release][releases] of Cluster API (CAPI) on GitHub.
- [Docker][docker] is required for the bootstrap cluster using `clusterctl`.
- [Kind][kind] can be used  to provide an initial management cluster for testing.
- [kubectl][kubectl] is required to access your workload clusters.

### Proxmox Requirements

Your Proxmox environment should be configured with a **DHCP server** in the primary VM Network for your workload Kubernetes clusters.
You will also need to configure a proxmox cluster containing the hosts (nodes) onto which the workload clusters will be provisioned. Every host
in the cluster will need access to shared storage, such as ZFS or LVM  in order to be able to make use of MachineDeployments and
high-availability control planes.

In addition, to use clusterctl, you should have a SSH public key that will be inserted into the node VMs for
administrative access, and a VM directory configured in Proxmox.

#### Uploading the machine images

It is required that machines provisioned by CAPPX have cloudinit, kubeadm and a container runtime pre-installed. You can
use one of the CAPPX machine images generated by SIG Cluster Lifecycle as a VM template.

The machine images are retrievable from public URLs. CAPPX currently supports machine images based on Ubuntu 20.04, 22.04 and
CentOS 8, 9. A list of published machine images is available [here][raw-vmdk-qcow2]. For this guide we'll be deploying Kubernetes
v1.26.3 on Ubuntu 22.04.

If you want to build your own image, take a look at the [image-builder][image-builder] project.

To reduce the time it takes to provisioning machines, linked clone mode is the default `cloneMode` for `proxmoxMachines` and is highly
recommended. To be able to use it, your VM templates require snapshots, for which we illustrate the process using the pve
command line tool, but can also be done via Proxmox.

```shell
# Re-mark the template as a VM
pve 
# Take a snapshot of the VM
pve
# Re-mark the VM as a template
pve
```

## Creating a test management cluster

**NOTE**: You will need an initial management cluster to run the Cluster API components. This can be any 1.16+ Kubernetes cluster.
If you are testing locally, you can use [Kind][kind] with the following command:

```shell
kind create cluster
```

#### Proxmox Credentials

In order for `clusterctl` to bootstrap a management cluster on Proxmox, it must be able to connect and authenticate to
Proxmox. Ensure you have credentials to your Proxmox server (user, password and server URL).

## Configuring and installing Cluster API Provider Proxmox in a management cluster

To initialize Cluster API Provider Proxmox, clusterctl requires the following variables, which should
be set in `~/.cluster-api/clusterctl.yaml` as the following:

``` yaml
## -- Controller settings -- ##
PROXMOX_USERNAME: "root"                    # The username used to access the remote Proxmox endpoint
PROXMOX_PASSWORD: "admin!23"                                  # The password used to access the remote Proxmox endpoint

## -- Required workload cluster default settings -- ##
PROXMOX_SERVER: "10.0.0.1"                                    # The Proxmox server IP or FQDN
PROXMOX_DATACENTER: "ross-proxmox"                         # The Proxmox datacenter to deploy the management cluster on
PROXMOX_DATASTORE: "local-zfs"                         # The Proxmox storage to deploy the management cluster on
PROXMOX_NETWORK: "vmbr0"                                 # The VM network to deploy the management cluster on
PROXMOX_RESOURCE_POOL: "infra"                          # The Proxmox pool for your VMs
PROXMOX_FOLDER: ""                                          # The VM folder for your VMs. Set to "" to use the root Proxmox folder
PROXMOX_TEMPLATE: "9020"                  # The VM template ID to use for your management cluster.
CONTROL_PLANE_ENDPOINT_IP: "10.50.150.250"                    # the IP that kube-vip is going to use as a control plane endpoint
VIP_NETWORK_INTERFACE: ""                               # The interface that kube-vip should apply the IP to. Omit to tell kube-vip to autodetect the interface.
PROXMOX_TLS_THUMBPRINT: ""                                 # sha1 thumbprint of the proxmox certificate: openssl x509 -sha1 -fingerprint -in ca.crt -noout
EXP_CLUSTER_RESOURCE_SET: "false"                              # This enables the ClusterResourceSet feature that we are using to deploy local-path-provisioner CSI
PROXMOX_SSH_AUTHORIZED_KEY: "ssh-rsa ..."              # The public ssh authorized key on all machines in this cluster.
                                                              #  Set to "" if you don't want to enable SSH, or are using another solution.
```

**NOTE**: Technically, SSH keys and Proxmox folders are optional, but optional template variables are not currently
supported by clusterctl. If you need to not set the Proxmox folder or SSH keys, then remove the appropriate fields after
running `clusterctl generate`.

the `CONTROL_PLANE_ENDPOINT_IP` is an IP that must be an IP on the same subnet as the control plane machines, it should be also an IP that is not part of your DHCP range

`CONTROL_PLANE_ENDPOINT_IP` is mandatory when you are using the default and the `external-loadbalancer` flavour

the `EXP_CLUSTER_RESOURCE_SET` is required if you want to deploy CSI using cluster resource sets (mandatory in the default flavor).

Setting `PROXMOX_USERNAME` and `PROXMOX_PASSWORD` is one way to manage identities. For the full set of options see [identity management](identity-management.md).

Once you have access to a management cluster, you can instantiate Cluster API with the following:

```shell
clusterctl init --infrastructure proxmox
```

## Creating a Proxmox-based workload cluster

The following command

```shell
$ clusterctl generate cluster proxmox-quickstart \
    --infrastructure proxmox \
    --kubernetes-version v1.26.3 \
    --control-plane-machine-count 1 \
    --worker-machine-count 3 > cluster.yaml

# Inspect and make any changes
$ vi cluster.yaml

# Create the workload cluster in the current namespace on the management cluster
$ kubectl apply -f cluster.yaml
```

## Accessing the workload cluster

The kubeconfig for the workload cluster will be stored in a secret, which can
be retrieved using:

``` shell
$ kubectl get secret/proxmox-quickstart-kubeconfig -o json \
  | jq -r .data.value \
  | base64 --decode \
  > ./proxmox-quickstart.kubeconfig
```

The kubeconfig can then be used to apply a CNI for networking, for example, Calico:

```shell
KUBECONFIG=proxmox-quickstart.kubeconfig kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

after that you should see your nodes turn into ready:

```shell
$ KUBECONFIG=proxmox-quickstart.kubeconfig kubectl get nodes
NAME                                                          STATUS     ROLES    AGE   VERSION
proxmox-quickstart-7yftd                                      Ready      master   14m   v1.26.3

```

## Custom cluster templates

the provided cluster templates are quickstarts. If you need anything specific that requires a more complex setup, we recommend to use custom templates:

```shell
$ clusterctl generate custom-cluster proxmox-quickstart \
    --infrastructure proxmox \
    --kubernetes-version v1.26.3 \
    --control-plane-machine-count 1 \
    --worker-machine-count 3 \
    --from ~/workspace/custom-cluster-template.yaml > custom-cluster.yaml
```

<!-- References -->
[cluster-api-book]: https://cluster-api.sigs.k8s.io/
[glossary-bootstrapping]: https://cluster-api.sigs.k8s.io/reference/glossary.html#bootstrap
[kind]: https://kind.sigs.k8s.io
[glossary-management-cluster]: https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/book/GLOSSARY.md#management-cluster
[releases]: https://github.com/kubernetes-sigs/cluster-api/releases
[docker]: https://docs.docker.com/glossary/?term=install
[kubectl]: https://kubernetes.io/docs/tasks/tools/install-kubectl/
[raw-vmdk-qcow2]: https://github.com/rosskirkpat/cluster-api-provider-proxmox/releases
[image-builder]: https://github.com/kubernetes-sigs/image-builder
